{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f3191-0951-47af-acc0-c5af6b034e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Tokenizer By Isaac Angulo Gomez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bf609a4-1e36-4605-864e-2ed705494969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "  Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Using cached regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "Installing collected packages: regex\n",
      "Successfully installed regex-2025.10.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c69e5f-bd30-448e-bf51-d2bfb82cec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.12/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests->transformers) (2025.6.15)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Using cached numpy-2.3.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Using cached safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: safetensors, numpy, hf-xet, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed filelock-3.20.0 fsspec-2025.10.0 hf-xet-1.2.0 huggingface-hub-0.36.0 numpy-2.3.4 safetensors-0.6.2 tokenizers-0.22.1 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e44e2669-aa96-42d2-82e6-76656d79990c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to detect whitespace, or accents\n",
    "import unicodedata\n",
    "\n",
    "#Used to support Unicode property classes\n",
    "import regex as re\n",
    "\n",
    "#Huggingface transformers BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#For Dictionary\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b01c033d-add0-4bf8-9eb2-61f5d7b1d6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "#downloads vocabulary\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#vocabulary size\n",
    "print(tok.vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7996deb-dbdf-4314-a18b-05fef66d91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEVER_SPLIT = {\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"}\n",
    "\n",
    "UNK = \"[UNK]\"\n",
    "CLS = \"[CLS]\"\n",
    "SEP = \"[SEP]\"\n",
    "PAD = \"[PAD]\"\n",
    "MASK = \"[MASK]\"\n",
    "\n",
    "#regex pattern for unicode punctuation\n",
    "_PUNC_RE = re.compile(r\"([\\p{P}])\")\n",
    "\n",
    "#Cleans the text\n",
    "\n",
    "#determines if character is a whitespace\n",
    "def _is_whitespace(ch):\n",
    "    return ch in (\" \", \"\\t\", \"\\n\", \"\\r\") or unicodedata.category(ch) == \"Zs\"\n",
    "\n",
    "#strips accents and returns string without accents\n",
    "def _strip_accents(text):\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    return \"\".join(ch for ch in text if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "#checks to see if character is a control or whitespace character\n",
    "def _is_control(ch):\n",
    "    cat = unicodedata.category(ch)\n",
    "    return (cat.startswith(\"C\") and ch not in (\"\\t\", \"\\n\", \"\\r\"))\n",
    "\n",
    "#rebuilds the text sting by removing control characters, null characters, or whitespaces, then returns it.\n",
    "def _clean_text(text):\n",
    "    out = []\n",
    "    for ch in text:\n",
    "        if ch == \"\\u0000\" or _is_control(ch):\n",
    "            continue\n",
    "        out.append(\" \" if _is_whitespace(ch) else ch)\n",
    "    return \"\".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a760c5bc-58df-40b8-9e1b-2fda57c46645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns an ordered vocab dictionary\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, token in enumerate(f):\n",
    "            token = token.rstrip(\"\\n\")\n",
    "            vocab[token] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74b457ce-e48a-4278-8f56-6168eb09cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizes usisng wodpiece tokenizer vocabulary\n",
    "class WordpieceTokenizer:\n",
    "\n",
    "    def __init__(self, vocab, unk_token = UNK, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self,token):\n",
    "        \n",
    "        if len(token) > self.max_input_chars_per_word:\n",
    "            return [self.unk_token]\n",
    "\n",
    "        sub_tokens = []\n",
    "        start = 0\n",
    "        while start < len(token):\n",
    "            end = len(token)\n",
    "            cur_substring = None\n",
    "\n",
    "            while start < end:\n",
    "                substring = token[start:end]\n",
    "\n",
    "                if start > 0:\n",
    "                    substring = \"##\" + substring\n",
    "\n",
    "                if substring in self.vocab:\n",
    "                    cur_substring = substring\n",
    "                    break\n",
    "\n",
    "                end -= 1\n",
    "\n",
    "            if cur_substring is None:\n",
    "                return [self.unk_token]\n",
    "\n",
    "            sub_tokens.append(cur_substring)\n",
    "\n",
    "            start = end\n",
    "            \n",
    "        return sub_tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57fd4662-b6bb-4a2c-af77-126bb1714854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer:\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case = True, never_split = None):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v:k for k, v in self.vocab.items()}\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(NEVER_SPLIT if never_split is None else never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(self.vocab, unk_token= UNK)\n",
    "        \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        text = _clean_text(text)\n",
    "\n",
    "        #make lower case and remove accents\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "            text = _strip_accents(text)\n",
    "\n",
    "        #split on whitespace and punctuation, keeping punctuation as token\n",
    "        tokens = []\n",
    "        for tok in text.strip().split():\n",
    "            if tok in self.never_split:\n",
    "                tokens.append(tok)\n",
    "                continue\n",
    "            parts = [p for p in _PUNC_RE.split(tok) if p and not p.isspace()]\n",
    "            tokens.extend(parts)\n",
    "\n",
    "        #wordpiece token list\n",
    "        wp_tokens = []\n",
    "        for t in tokens:\n",
    "            if t in self.never_split:\n",
    "                wp_tokens.append(t)\n",
    "            else:\n",
    "                wp_tokens.extend(self.wordpiece_tokenizer.tokenize(t))\n",
    "\n",
    "        return wp_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        unk_id = self.vocab.get(UNK)\n",
    "\n",
    "        return [self.vocab.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.inv_vocab[i] for i in ids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5f8ac26-b095-4565-ad09-4f027fd56acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_from_tokens(tokens_a, tokens_b = None, max_len = 512, pad_to_max = True, pad_token = PAD):\n",
    "    tokens = [CLS] + tokens_a + [SEP]\n",
    "\n",
    "    token_type_ids = [0] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [SEP]\n",
    "        token_type_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "        token_type_ids = token_type_ids[:max_len]\n",
    "\n",
    "    attention_mask = [1] * len(tokens)\n",
    "\n",
    "    if pad_to_max and len(tokens) < max_len:\n",
    "        pad_len = max_len - len(tokens)\n",
    "        tokens += [pad_token] * pad_len\n",
    "        token_type_ids += [0] * pad_len\n",
    "        attention_mask += [0] * pad_len\n",
    "\n",
    "    return tokens, token_type_ids, attention_mask\n",
    "\n",
    "def build_inputs_from_texts(tokenizer, text_a, text_b = None, max_len = 512):\n",
    "    ta = tokenizer.tokenize(text_a)\n",
    "    tb = tokenizer.tokenize(text_b) if text_b is not None else None\n",
    "    tokens, token_type_ids, attention_mask = build_inputs_from_tokens(ta, tb, max_len=max_len)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return dict(\n",
    "        input_ids = input_ids,\n",
    "        token_type_ids = token_type_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        tokens = tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22a739b3-e574-4415-a12d-a90dac363afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', '##believable', '!', 'cats', 'are', '##n', \"'\", 't', 'dogs', '.']\n",
      "{'input_ids': [2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['[CLS]', 'un', '##believable', '!', 'cats', 'are', '##n', \"'\", 't', 'dogs', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']}\n"
     ]
    }
   ],
   "source": [
    "#Testing for Toekenization using fake vocab\n",
    "\n",
    "fake_vocab = collections.OrderedDict({\n",
    "    PAD:0, UNK:1, CLS:2, SEP:3, MASK:4,\n",
    "    \"un\":5, \"##believable\":6, \"!\":7,\n",
    "    \"cats\":8, \"are\":9, \"##n\":10, \"'\":11, \"t\":12,\n",
    "    \"dogs\":13, \".\":14\n",
    "})\n",
    "\n",
    "# Write fake vocab to temporary file\n",
    "with open(\"fake_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in fake_vocab.keys():\n",
    "        f.write(tok + \"\\n\")\n",
    "\n",
    "#Create teokenizer and test with String\n",
    "tok = FullTokenizer(\"fake_vocab.txt\", do_lower_case=True)\n",
    "print(tok.tokenize(\"Unbelievable! Cats aren't Dogs.\"))\n",
    "print(build_inputs_from_texts(tok, \"Unbelievable! Cats aren't Dogs.\", max_len=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94b1ba8c-cce8-4af9-bbdf-234ae877eb58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/conda/lib/python3.12/site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /opt/conda/lib/python3.12/site-packages (from torch) (2025.10.0)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.5 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvshmem-cu12==3.3.20 (from torch)\n",
      "  Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.5.0 (from torch)\n",
      "  Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Using cached torch-2.9.0-cp312-cp312-manylinux_2_28_x86_64.whl (899.7 MB)\n",
      "Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.3 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "Using cached nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (124.7 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Using cached triton-3.5.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (170.5 MB)\n",
      "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20/20\u001b[0m [torch]m19/20\u001b[0m [torch]-cusolver-cu12]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.0 triton-3.5.0\n"
     ]
    }
   ],
   "source": [
    "# Full Pre-Training Builder By Isaac Angulo Gomez\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83cff478-cd92-4031-8275-c37ce386667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import random\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f57d74f3-2ce9-4ca8-8aca-5f6926f4dc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Instance:\n",
    "    input_ids: List[int]\n",
    "    token_type_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    mlm_labels: List[int]\n",
    "    nsp_lavel: int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ddeef048-b472-4197-a1cd-94b651b4cf17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shortern sequence by removing tokens from a or b until it fits the max length\n",
    "def _truncate_seq_pair(tokens_a: List[int], tokens_b: List[int], max_len: int) -> Tuple[List[int], List[int]]:\n",
    "    while len(token_a) + len(token_b) > max_len:\n",
    "        if len(token_a) > len(token_b):\n",
    "            if random.random() < 0.5:\n",
    "                tokens_a.pop(0)\n",
    "            else:\n",
    "                token_a.pop()\n",
    "        else:\n",
    "            if random.random() < 0.5:\n",
    "                token_b.pop(0)\n",
    "            else:\n",
    "                token_b.pop()\n",
    "    return tokens_a, tokens_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a259e427-a296-4ae0-b4a3-376aabcde861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Masks 15% of tokens\n",
    "def _create_masked_lm(tokens, mask_token_in, pad_token_id, special_token_ids, vocab_size, mask_prob=.015):\n",
    "    #get positions of tokesn that are not special tokens or pad tokens\n",
    "    cand_positions = [i for i, t in enumerate(tokens) \n",
    "                      if t not in special_tokens_ids and t != pad_token_id]\n",
    "    \n",
    "    num_to_mask = max(1, int(round(len(cand_positions) * mask_prob))) if cand_positions else 0\n",
    "    \n",
    "    masked_positions = set(random.sample(cand_positions, num_to_mask)) if num_to_mask else set()\n",
    "\n",
    "    mlm_labels = [-100] * len(tokens)\n",
    "    new_tokens = list(tokens)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        for i in masked_positions:\n",
    "            mlm_labels[i] = tokens[i]\n",
    "            r = random.random()\n",
    "            if r < 0.8:\n",
    "                new_toknes[i] = mask_token_id\n",
    "            elif r < 0.9:\n",
    "                #Replace with random token\n",
    "                for _ in range(10):\n",
    "                    cand = random.randint(0, vocab_size - 1)\n",
    "                    if cand not in special_token_ids and cand != pad_token_id:\n",
    "                        new_tokens[i] = cand\n",
    "                        break\n",
    "                else:\n",
    "                    new_tokens[i] = tokens[i]\n",
    "            else:\n",
    "                #Keep original\n",
    "                new_tokens[i] = tokens[i]\n",
    "        return new_tokens, mlm_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb41b113-5920-4d75-a8a8-110e1ca3e7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adds [CLS] and [SEP] and creates token_type_ids\n",
    "def _pack(tokens_a, tokens_b, special_ids):\n",
    "    cls_id, sep_id = special_ids[\"cls\"], special[\"sep\"]\n",
    "    input_ids = [cls_id] + tokens_a + [sep_id] + tokens_b + [sep_id]\n",
    "    token_type_ids = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "    return input_ids, token_type_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf3a6d1-92d8-44b8-90a4-39afb9382c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds IsNext pair from document\n",
    "def _make_isnext_pair(doc_sents, max_seq_len_no_special, shor_seq_prob):\n",
    "    \n",
    "    #Split artificially if len = 1\n",
    "    if len(doc_sents) == 1:\n",
    "        s = doc_sents[0]\n",
    "        mid = max(1, len(s)//2)\n",
    "        \n",
    "        return s[:mid], s[mid:], 1\n",
    "\n",
    "    #randomly split within document\n",
    "    start = random.randrange(0,len(doc_sents)-1)\n",
    "    token_a = list(doc_sents[start])\n",
    "    i = start + 1\n",
    "    while i < len(doc_sents)-1 and len(tones_a) < max_seq_len_no_special// 2 and random.random() < 0.5:\n",
    "        tokens_a += doc_sents[i]\n",
    "        i += 1\n",
    "\n",
    "    tokens_b = list(doc-sents[i])\n",
    "    i += i\n",
    "    while i < len(doc_sents) and (len(tokens_a) + len(tokens_b)) < max_seq_len_no_special and random.random() < 0.7:\n",
    "        tokens_b += doc_sents[i]\n",
    "        i += 1\n",
    "\n",
    "    if random.randm() < short_seq_prob:\n",
    "        tarket = random.randint(2, max(2, max_seq_len_no_special//2))\n",
    "        tokesn_a = tokens_a[:target//2]\n",
    "        tokens_b = tokens_b[:target - len(tokens_a)]\n",
    "\n",
    "    return tokens_a, tokens_b, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "972ab0b6-7c87-4728-9127-cffea8c06b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds pair of sentences from two different documents so they dont go together\n",
    "def _make_notnext_pair(all_docs, cur_doc_idx, max_seq_len_no_special, short_seq_prob):\n",
    "    doc_a = all_docs[cur_doc_idx]\n",
    "    start = random.randrange(0, len(doc_a))\n",
    "    tokens_a = list(doc_a[start])\n",
    "    i = start + 1\n",
    "\n",
    "    while i < len(doc_a) and len(toknes) < max_seq_len_no_special//2 and random.random() < 0.5:\n",
    "        tokens_a += doc_a[i]\n",
    "        i += 1\n",
    "\n",
    "    #pick pair from another document\n",
    "    other_idx = cur_doc_idx\n",
    "\n",
    "    if len(all_docs) > 1:\n",
    "        while other_idx == cur_doc_idx: \n",
    "            other_idx = random.randrange(0, len(all_docs))\n",
    "\n",
    "    doc_b = all_docs[other_idx]\n",
    "\n",
    "    start_b = random.randrange(0, len(doc_b))\n",
    "    tokens_b = list(doc_b[start_b])\n",
    "\n",
    "    j = start_b + 1\n",
    "\n",
    "    while j < len(doc_b) and (len(tokens_a) + len(token_b)) < max_seq_len_no_special and random.random() < 0.7:\n",
    "        tokens_b += doc_b[j]\n",
    "        j+= 1\n",
    "\n",
    "    if random.random() < short_seq_prob:\n",
    "        target = random.randint(2, max(2, max_seq_len_no_sepcial//2))\n",
    "        token_a = tokens_a[:target//2]\n",
    "        target_b = tokens_b[:target - len(tokens_a)]\n",
    "\n",
    "    return tokens_a, tokens_b, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16dbb399-469f-4276-b6e1-466a7e675f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Builds MLM + NSP istances for all documents\n",
    "def build_pretraining_instance(tokenized_documents, special_ids, vocab_size, max_seq_len=128, short_seq_prob = 0.1, nsp_prob=0.5, mask_prob=0.15, seed=42):\n",
    "    rng_state = random.getstate()\n",
    "    random.seed(seed)\n",
    "    instances = []\n",
    "    max_seq_len_no_special = max_seq_len - 3\n",
    "    special_token_ids = {special_ids[\"cls\"], special_ids[\"sep\"], special_ids[\"pad\"], special_ids[\"mask\"]}\n",
    "\n",
    "    for d_idx, doc in enumerate(tokenized_documents):\n",
    "        if not doc:\n",
    "            continue\n",
    "\n",
    "        for _ in range(max(1, len(doc))):\n",
    "            is_next = random.random() < nsp_prob\n",
    "\n",
    "            if(is_next):\n",
    "                a, b, nsp = _make_isnext_pair(doc, max_seq_len_no_special, short_seq_prob)\n",
    "            else:\n",
    "                a, b, nsp = _make_notnext_pair(tokenized_documents, d_idx, max_seq_len_no_special, short_seq_prob)\n",
    "\n",
    "\n",
    "            a, b = _truncate_seq_pair(a,b , max_seq_len_no_special)\n",
    "            input_ids, token_type_ids = _pack(a,b, special_ids)\n",
    "            attention_mas = [1] * len(input_ids)\n",
    "\n",
    "            masked_ids, mlm_labels = _create_masked_lm(input_ids, masked_token_ids = special[\"mask\"], pad_token_id = special_ids[\"pad\"], specia_token_ids = special_token_ids, vocab_size = vocab_size, mask_prob = mask_prob)\n",
    "\n",
    "            inst = Instantiate(masked_ids, token_type_ids, attention_mask, mlm_labels, nsp)\n",
    "            instances.append(inst)\n",
    "\n",
    "        random.setstate(rng_state)\n",
    "        return instances\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a91137b1-846e-4162-9525-cf028368393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper for pretaining instances\n",
    "class BertPretrainDatase(torch.utils.data.Dataset):\n",
    "    def __init__(self, instances,pad_token_id,max_seq_len):\n",
    "        self.instances = instances\n",
    "        self.pad = pad_toke_id\n",
    "        self.max_len = max_seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.instances)\n",
    "\n",
    "    def __getitem__(self, idex):\n",
    "        inst = self.instance[idx]\n",
    "        return {\n",
    "            \"inputs_ids\": torch.tensor(inst.input_ids, dtype = torch.long),\n",
    "            \"token_type_ids\": torch.tensor(inst.token_type_ids, dtype = torch.long),\n",
    "            \"attention_mask\": torch.tensor(inst.attention_mask, dtype=torch.long),\n",
    "            \"mlm_labels\": torch.tensor(inst.mlm_labels, dtype=torch.long),\n",
    "            \"nsp_label\": torch.tensor(inst.nsp_label, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d82345e-a5ca-4254-b174-ef0227c90948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pad batch of variable length examples into uniform tensors\n",
    "def bert_collate_fn(batch, pad_token_id, max_seq_len):\n",
    "    bsz = len(batch)\n",
    "    out = {}\n",
    "    keys = [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"mlm_labels\"]\n",
    "\n",
    "    for k in keys:\n",
    "        if k == \"mlm_labels\":\n",
    "            paded = torch.full((bsz, max_seq_len), -100, dtype=torch.long)\n",
    "        elif k == \"input_ids\":\n",
    "            padded = torch.full((bsz, max_seq_len), pad_token_id, dtype = torch.long)\n",
    "        else: \n",
    "            padded = torch.zeros((bsz, max_seq_len), dtype=torch.long)\n",
    "\n",
    "        for i, items in enumerate(batch):\n",
    "            x = item[k]\n",
    "            L = min(len(x), max_seq_len)\n",
    "            padded[i, :L] = x[:L]\n",
    "\n",
    "        out[k] = padded\n",
    "\n",
    "    out[\"nsp_label\"] = torch.stach([item[\"nsp_label\"] for item in batch]).view(-1)\n",
    "    out[\"attention_mask\"] = (out[\"input_ids\"] != pad_token_id).long()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af98f39-f709-439c-89b2-2079601dfa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
