{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ec29760-e583-4892-9161-666deb3f4640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.12/site-packages (from nltk) (8.3.0)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.12/site-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.12/site-packages (from nltk) (2025.10.23)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "d78feee1-d618-438f-93c1-53ac4175a901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk, ssl\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "for res in [\"punkt\", \"punkt_tab\"]:\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{res}\")\n",
    "    except LookupError:\n",
    "        nltk.download(res)\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "31f5c9f5-3c3d-42b0-84db-54c3dbf6e784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert-base-uncased/tokenizer_config.json',\n",
       " './bert-base-uncased/special_tokens_map.json',\n",
       " './bert-base-uncased/vocab.txt',\n",
       " './bert-base-uncased/added_tokens.json',\n",
       " './bert-base-uncased/tokenizer.json')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok_hf = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tok_hf.save_pretrained(\"./bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7b3af6db-1279-4748-b1b7-d8dc3ece5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "from Tokenizer import FullTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "82490bd5-f783-47d1-a76b-1de6cd2ea560",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizer Object\n",
    "my_tok = FullTokenizer(\"./bert-base-uncased/vocab.txt\", do_lower_case = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "be4c7b0e-5624-44c3-8080-7cdf8d382152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tokenized_documents(raw_docs, tokenizer):\n",
    "    out = []\n",
    "    \n",
    "    for doc in raw_docs:\n",
    "        sents = sent_tokenize(doc)\n",
    "        sent_ids = []\n",
    "        \n",
    "        for s in sents:\n",
    "            toks = tokenizer.tokenize(s)\n",
    "            ids = tokenizer.convert_tokens_to_ids(toks)\n",
    "    \n",
    "            if ids:\n",
    "                sent_ids.append(ids)\n",
    "    \n",
    "        if sent_ids:\n",
    "            out.append(sent_ids)\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "45043b29-f1de-4b89-b77a-6607194dded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "special_ids = {\n",
    "    \"cls\": tok_hf.cls_token_id,\n",
    "    \"sep\": tok_hf.sep_token_id,\n",
    "    \"pad\": tok_hf.pad_token_id if tok_hf.pad_token_id is not None else 0,\n",
    "    \"mask\": tok_hf.mask_token_id,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b3238843-dde2-45e7-ab57-e859bdc8dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = tok_hf.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dbe285df-9957-4f0a-bbd7-f078015cca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from bert_pretrain_data import build_pretraining_instances, BertPretrainDataset, bert_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "69a2d1f6-8849-44e1-98d1-8731c0218596",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = build_pretraining_instances(tokenized_documents, special_ids = special_ids, vocab_size = vocab_size, max_seq_len = 128, short_seq_prob = 0.1, nsp_prob = 0.5, mask_prob = 0.15, seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "16687405-0e7a-46be-a2f6-414b30f69c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- Instance 0 ----\n",
      "NSP label: IsNext\n",
      "len(input_ids): 12\n",
      "Tokens: [CLS] hello world . [SEP] this *[MASK]* a test document . [SEP]\n",
      "\n",
      "---- Instance 1 ----\n",
      "NSP label: NotNext\n",
      "len(input_ids): 23\n",
      "Tokens: [CLS] hello world . this *[MASK]* *[MASK]* test document . [SEP] bert *[MASK]* - training uses ml ##m and ns ##p . [SEP]\n",
      "\n",
      "---- Instance 2 ----\n",
      "NSP label: NotNext\n",
      "len(input_ids): 23\n",
      "Tokens: [CLS] bert *[MASK]* *-* training *[MASK]* ml ##m and ns ##p . [SEP] hello world . this is a test document . [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Examples of first 3 instances\n",
    "for i, inst in enumerate(instances[:3]):\n",
    "    print(f\"---- Instance {i} ----\")\n",
    "    print(\"NSP label:\", \"IsNext\" if inst.nsp_label == 1 else \"NotNext\")\n",
    "    print(\"len(input_ids):\", len(inst.input_ids))\n",
    "    \n",
    "    # reconstruct token strings from IDs (for easier human reading)\n",
    "    tokens = [tok_hf.convert_ids_to_tokens([tid])[0] for tid in inst.input_ids]\n",
    "    masked_tokens = []\n",
    "    for t, label in zip(tokens, inst.mlm_labels):\n",
    "        if label != -100:\n",
    "            # mark masked tokens visibly\n",
    "            masked_tokens.append(f\"*{t}*\")\n",
    "        else:\n",
    "            masked_tokens.append(t)\n",
    "    print(\"Tokens:\", \" \".join(masked_tokens))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "7f6d63e6-5188-478d-af86-b19fec37ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = \"vocab.txt\"  # path to downloaded vocab\n",
    "tokenizer = FullTokenizer(vocab_file=vocab_file, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f2e6a0d9-084a-4af0-ac3c-4b7533e3acc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n",
      "[101, 1996, 4248, 2829, 102]\n",
      "30522\n"
     ]
    }
   ],
   "source": [
    "#Check tokenizer using vocab\n",
    "print(tokenizer.tokenize(\"The quick brown fox jumped over the lazy dog.\"))\n",
    "print(tokenizer.convert_tokens_to_ids([\"[CLS]\", \"the\", \"quick\", \"brown\", \"[SEP]\"]))\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd391895-9706-4c34-a3f3-a0590f1fab03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
