{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bf609a4-1e36-4605-864e-2ed705494969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting regex\n",
      "  Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Downloading regex-2025.10.23-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (803 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.4/803.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: regex\n",
      "Successfully installed regex-2025.10.23\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5c69e5f-bd30-448e-bf51-d2bfb82cec2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.35.3-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from transformers) (2025.10.23)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.12/site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.12/site-packages (from requests->transformers) (2025.7.9)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.35.3-py3-none-any.whl (564 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.10-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, safetensors, hf-xet, huggingface-hub, tokenizers, transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [transformers][0m [transformers]ub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed hf-xet-1.1.10 huggingface-hub-0.35.3 safetensors-0.6.2 tokenizers-0.22.1 tqdm-4.67.1 transformers-4.57.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e44e2669-aa96-42d2-82e6-76656d79990c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Used to detect whitespace, or accents\n",
    "import unicodedata\n",
    "\n",
    "#Used to support Unicode property classes\n",
    "import regex as re\n",
    "\n",
    "#Huggingface transformers BertTokenizer\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#For Dictionary\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01c033d-add0-4bf8-9eb2-61f5d7b1d6f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "#downloads vocabulary\n",
    "tok = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "#vocabulary size\n",
    "print(tok.vocab_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7996deb-dbdf-4314-a18b-05fef66d91b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEVER_SPLIT = {\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"}\n",
    "\n",
    "UNK = \"[UNK]\"\n",
    "CLS = \"[CLS]\"\n",
    "SEP = \"[SEP]\"\n",
    "PAD = \"[PAD]\"\n",
    "MASK = \"[MASK]\"\n",
    "\n",
    "#regex pattern for unicode punctuation\n",
    "_PUNC_RE = re.compile(r\"([\\p{P}])\")\n",
    "\n",
    "#Cleans the text\n",
    "\n",
    "#determines if character is a whitespace\n",
    "def _is_whitespace(ch):\n",
    "    return ch in (\" \", \"\\t\", \"\\n\", \"\\r\") or unicodedata.category(ch) == \"Zs\"\n",
    "\n",
    "#strips accents and returns string without accents\n",
    "def _strip_accents(text):\n",
    "    text = unicodedata.normalize(\"NFD\", text)\n",
    "    return \"\".join(ch for ch in text if unicodedata.category(ch) != \"Mn\")\n",
    "\n",
    "#checks to see if character is a control or whitespace character\n",
    "def _is_control(ch):\n",
    "    cat = unicodedata.category(ch)\n",
    "    return (cat.startswith(\"C\") and ch not in (\"\\t\", \"\\n\", \"\\r\"))\n",
    "\n",
    "#rebuilds the text sting by removing control characters, null characters, or whitespaces, then returns it.\n",
    "def _clean_text(text):\n",
    "    out = []\n",
    "    for ch in text:\n",
    "        if ch == \"\\u0000\" or _is_control(ch):\n",
    "            continue\n",
    "        out.append(\" \" if _is_whitespace(ch) else ch)\n",
    "    return \"\".join(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a760c5bc-58df-40b8-9e1b-2fda57c46645",
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns an ordered vocab dictionary\n",
    "def load_vocab(vocab_file):\n",
    "    vocab = collections.OrderedDict()\n",
    "    with open(vocab_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for i, token in enumerate(f):\n",
    "            token = token.rstrip(\"\\n\")\n",
    "            vocab[token] = i\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b457ce-e48a-4278-8f56-6168eb09cd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizes usisng wodpiece tokenizer vocabulary\n",
    "class WordpieceTokenizer:\n",
    "\n",
    "    def __init__(self, vocab, unk_token = UNK, max_input_chars_per_word=100):\n",
    "        self.vocab = vocab\n",
    "        self.unk_token = unk_token\n",
    "        self.max_input_chars_per_word = max_input_chars_per_word\n",
    "\n",
    "    def tokenize(self,token):\n",
    "        \n",
    "        if len(token) > self.max_input_chars_per_word:\n",
    "            return [self.unk_token]\n",
    "\n",
    "        sub_tokens = []\n",
    "        start = 0\n",
    "        while start < len(token):\n",
    "            end = len(token)\n",
    "            cur_substring = None\n",
    "\n",
    "            while start < end:\n",
    "                substring = token[start:end]\n",
    "\n",
    "                if start > 0:\n",
    "                    substring = \"##\" + substring\n",
    "\n",
    "                if substring in self.vocab:\n",
    "                    cur_substring = substring\n",
    "                    break\n",
    "\n",
    "                end -= 1\n",
    "\n",
    "            if cur_substring is None:\n",
    "                return [self.unk_token]\n",
    "\n",
    "            sub_tokens.append(cur_substring)\n",
    "\n",
    "            start = end\n",
    "            \n",
    "        return sub_tokens\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fd4662-b6bb-4a2c-af77-126bb1714854",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullTokenizer:\n",
    "\n",
    "    def __init__(self, vocab_file, do_lower_case = True, never_split = None):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.inv_vocab = {v:k for k, v in self.vocab.items()}\n",
    "        self.do_lower_case = do_lower_case\n",
    "        self.never_split = set(NEVER_SPLIT if never_split is None else never_split)\n",
    "        self.wordpiece_tokenizer = WordpieceTokenizer(self.vocab, unk_token= UNK)\n",
    "        \n",
    "\n",
    "    def tokenize(self, text):\n",
    "        if not text:\n",
    "            return []\n",
    "\n",
    "        text = _clean_text(text)\n",
    "\n",
    "        #make lower case and remove accents\n",
    "        if self.do_lower_case:\n",
    "            text = text.lower()\n",
    "            text = _strip_accents(text)\n",
    "\n",
    "        #split on whitespace and punctuation, keeping punctuation as token\n",
    "        tokens = []\n",
    "        for tok in text.strip().split():\n",
    "            if tok in self.never_split:\n",
    "                tokens.append(tok)\n",
    "                continue\n",
    "            parts = [p for p in _PUNC_RE.split(tok) if p and not p.isspace()]\n",
    "            tokens.extend(parts)\n",
    "\n",
    "        #wordpiece token list\n",
    "        wp_tokens = []\n",
    "        for t in tokens:\n",
    "            if t in self.never_split:\n",
    "                wp_tokens.append(t)\n",
    "            else:\n",
    "                wp_tokens.extend(self.wordpiece_tokenizer.tokenize(t))\n",
    "\n",
    "        return wp_tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        unk_id = self.vocab.get(UNK)\n",
    "\n",
    "        return [self.vocab.get(t, unk_id) for t in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.inv_vocab[i] for i in ids]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8ac26-b095-4565-ad09-4f027fd56acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_inputs_from_tokens(tokens_a, tokens_b = None, max_len = 512, pad_to_max = True, pad_token = PAD):\n",
    "    tokens = [CLS] + tokens_a + [SEP]\n",
    "\n",
    "    token_type_ids = [0] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [SEP]\n",
    "        token_type_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "        token_type_ids = token_type_ids[:max_len]\n",
    "\n",
    "    attention_mask = [1] * len(tokens)\n",
    "\n",
    "    if pad_to_max and len(tokens) < max_len:\n",
    "        pad_len = max_len - len(tokens)\n",
    "        tokens += [pad_token] * pad_len\n",
    "        token_type_ids += [0] * pad_len\n",
    "        attention_mask += [0] * pad_len\n",
    "\n",
    "    return tokens, token_type_ids, attention_mask\n",
    "\n",
    "def build_inputs_from_texts(tokenizer, text_a, text_b = None, max_len = 512):\n",
    "    ta = tokenizer.tokenize(text_a)\n",
    "    tb = tokenizer.tokenize(text_b) if text_b is not None else None\n",
    "    tokens, token_type_ids, attention_mask = build_inputs_from_tokens(ta, tb, max_len=max_len)\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    return dict(\n",
    "        input_ids = input_ids,\n",
    "        token_type_ids = token_type_ids,\n",
    "        attention_mask = attention_mask,\n",
    "        tokens = tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a739b3-e574-4415-a12d-a90dac363afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['un', '##believable', '!', 'cats', 'are', '##n', \"'\", 't', 'dogs', '.']\n",
      "{'input_ids': [2, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 3, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0], 'tokens': ['[CLS]', 'un', '##believable', '!', 'cats', 'are', '##n', \"'\", 't', 'dogs', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']}\n"
     ]
    }
   ],
   "source": [
    "#Testing for Toekenization using fake vocab\n",
    "\n",
    "fake_vocab = collections.OrderedDict({\n",
    "    PAD:0, UNK:1, CLS:2, SEP:3, MASK:4,\n",
    "    \"un\":5, \"##believable\":6, \"!\":7,\n",
    "    \"cats\":8, \"are\":9, \"##n\":10, \"'\":11, \"t\":12,\n",
    "    \"dogs\":13, \".\":14\n",
    "})\n",
    "\n",
    "# Write fake vocab to temporary file\n",
    "with open(\"fake_vocab.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tok in fake_vocab.keys():\n",
    "        f.write(tok + \"\\n\")\n",
    "\n",
    "#Create teokenizer and test with String\n",
    "tok = FullTokenizer(\"fake_vocab.txt\", do_lower_case=True)\n",
    "print(tok.tokenize(\"Unbelievable! Cats aren't Dogs.\"))\n",
    "print(build_inputs_from_texts(tok, \"Unbelievable! Cats aren't Dogs.\", max_len=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1ba8c-cce8-4af9-bbdf-234ae877eb58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cff478-cd92-4031-8275-c37ce386667d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57d74f3-2ce9-4ca8-8aca-5f6926f4dc24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
